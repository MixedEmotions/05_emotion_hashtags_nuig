{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import logging\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from senpy.plugins import EmotionPlugin, SenpyPlugin\n",
    "from senpy.models import Results, EmotionSet, Entry, Emotion\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import codecs, csv, re, nltk\n",
    "import numpy as np\n",
    "import math, itertools\n",
    "from drevicko.twitter_regexes import cleanString, setupRegexes, tweetPreprocessor\n",
    "import preprocess_twitter\n",
    "from collections import defaultdict\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.tokenize.casual as casual\n",
    "\n",
    "import gzip\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class hashTagProba(EmotionPlugin):\n",
    "    \n",
    "    def __init__(self, info, *args, **kwargs):\n",
    "        super(hashTagProba, self).__init__(info, *args, **kwargs)\n",
    "        self.name = info['name']\n",
    "        self.id = info['module']\n",
    "        self._info = info\n",
    "        local_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        self.testing = False\n",
    "        self._categories = {'sadness':[],\n",
    "                            'disgust':[],\n",
    "                            'surprise':[],\n",
    "                            'anger':[],\n",
    "                            'fear':[],\n",
    "                            'joy':[]}   \n",
    "        self._maxlen = 65\n",
    "                \n",
    "        self._savedModelPath = local_path + \"/classifiers/LSTM/hashTagProba\"\n",
    "        self._path_wordembeddings = os.path.dirname(local_path) + '/glove.twitter.27B.100d.txt.gz'\n",
    "        \n",
    "        self.emoNames = ['sadness', 'disgust', 'surprise', 'anger', 'fear', 'joy'] \n",
    "#         self.emoNames = ['sadness', 'disgust', 'sadness', 'joy', 'fear', 'anger']\n",
    "#         self.emoNames = ['anger','fear','joy','sadness'] \n",
    "    def _load_unique_tokens(self, filename = 'wordFrequencies.dump'):    \n",
    "        return joblib.load(filename)    \n",
    "        \n",
    "\n",
    "    def activate(self, *args, **kwargs):\n",
    "        \n",
    "        np.random.seed(1337)\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._hashTagDLModel = self._load_model_and_weights(self._savedModelPath)  \n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _hashTagDLModel\"))\n",
    "        \n",
    "        if self.testing:\n",
    "            st = datetime.now()\n",
    "            self._wordFrequencies = self._load_unique_tokens(\n",
    "                filename = os.path.join(\n",
    "                    os.path.dirname(os.path.dirname(__file__)), 'hashTagClassification', 'wordFrequencies.dump'))\n",
    "            logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordFrequencies\"))\n",
    "        else:\n",
    "            self._wordFrequencies = None        \n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._Dictionary, self._Indices = self._load_original_vectors(\n",
    "            filename = self._path_wordembeddings, \n",
    "            sep = ' ',\n",
    "            wordFrequencies = self._wordFrequencies, \n",
    "            zipped = True) # leave wordFrequencies=None for loading the entire WE file\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordEmbeddings\"))\n",
    "        \n",
    "        logger.info(\"%s plugin is ready to go!\" % self.name)\n",
    "        \n",
    "    def deactivate(self, *args, **kwargs):\n",
    "        try:\n",
    "            logger.info(\"%s plugin is being deactivated...\" % self.name)\n",
    "        except Exception:\n",
    "            print(\"Exception in logger while reporting deactivation of %s\" % self.name)\n",
    "\n",
    "    # CUSTOM FUNCTIONS\n",
    "    \n",
    "    def _load_model_and_weights(self, filename):\n",
    "        with open(filename+'.json', 'r') as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            \n",
    "        loaded_model.load_weights(filename+'.h5')\n",
    "        \n",
    "        return loaded_model\n",
    "    \n",
    "    def _lists_to_vectors(self, text):\n",
    "        train_sequences = [self._text_to_sequence(text)]  \n",
    "        X = sequence.pad_sequences(train_sequences, maxlen=self._maxlen)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _text_to_sequence(self,text):\n",
    "        train_sequence = []\n",
    "        for token in text.split():\n",
    "            try:\n",
    "                train_sequence.append(self._Indices[token])\n",
    "            except:\n",
    "                train_sequence.append(0)\n",
    "        train_sequence.extend([0]*( self._maxlen-len(train_sequence)) )\n",
    "        return np.array(train_sequence)  \n",
    "    \n",
    "    def _text_preprocessor(self, text):        \n",
    "        text = preprocess_twitter.tokenize(text)        \n",
    "        text = casual.reduce_lengthening(text)\n",
    "        text = cleanString(setupRegexes('twitterProAna'),text)  \n",
    "        text = ' '.join([span for notentity,span in tweetPreprocessor(text, (\"urls\", \"users\", \"lists\")) if notentity]) \n",
    "        text = text.replace('\\t','')\n",
    "        text = text.replace('< ','<').replace(' >','>')\n",
    "        text = text.replace('):', '<sadface>').replace('(:', '<smile>')\n",
    "        text = text.replace(\" 't\", \"t\")#.replace(\"#\", \"\")\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def tokenise_tweet(text):\n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        text = preprocess_tweet(text)     \n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    \n",
    "    def _load_original_vectors(self, filename = 'glove.27B.100d.txt', sep = ' ', wordFrequencies = None, zipped = False):\n",
    "       \n",
    "        def __read_file(f):\n",
    "            Dictionary, Indices  = {},{}\n",
    "            i = 1\n",
    "            for line in f:\n",
    "                line_d = line.decode('utf-8').split(sep)\n",
    "\n",
    "                token = line_d[0]\n",
    "                token_vector = np.array(line_d[1:], dtype = 'float32')   \n",
    "                if(wordFrequencies):\n",
    "                    if(token in wordFrequencies):                \n",
    "                        Dictionary[token] = token_vector\n",
    "                        Indices.update({token:i})\n",
    "                        i+=1\n",
    "                else:\n",
    "                    Dictionary[token] = token_vector\n",
    "                    Indices.update({token:i})\n",
    "                    i+=1\n",
    "            return Dictionary, Indices\n",
    "            \n",
    "        if zipped:\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                return __read_file(f)\n",
    "        else:\n",
    "            with open(filename, 'rb') as f:\n",
    "                return __read_file(f)\n",
    "            \n",
    "\n",
    "    def _extract_features(self, X):\n",
    "        if self._ESTIMATION == 'Probabilities':            \n",
    "            y_predict = np.array(self._hashTagDLModel.predict(X))[0]            \n",
    "        else:\n",
    "            blank = [0] * len(self.emoNames)\n",
    "            for i,pred in enumerate(self._hashTagDLModel.predict_classes(X)):\n",
    "                print(i,pred)\n",
    "                blank[pred] = 1\n",
    "            print(blank)\n",
    "            y_predict = np.array(blank)\n",
    "            \n",
    "        feature_set = {emo: y_ for emo, y_ in zip(self.emoNames, y_predict)}\n",
    "            \n",
    "        return feature_set       \n",
    "    \n",
    "    \n",
    "    def analyse(self, **params):\n",
    "        \n",
    "        logger.debug(\"Hashtag LSTM Analysing with params {}\".format(params))          \n",
    "                  \n",
    "        text_input = params.get(\"input\", None)        \n",
    "        self._ESTIMATION = params.get(\"estimation\", 'Probabilities')\n",
    "        \n",
    "        \n",
    "        # EXTRACTING FEATURES\n",
    "        \n",
    "        text = self._text_preprocessor(text_input)    \n",
    "        \n",
    "        X = self._lists_to_vectors(text = text)   \n",
    "        feature_text = self._extract_features(X = X)    \n",
    "        \n",
    "        \n",
    "        # GENERATING RESPONSE   \n",
    "        \n",
    "        response = Results()\n",
    "       \n",
    "        entry = Entry()\n",
    "        entry.nif__isString = text_input\n",
    "        \n",
    "        emotionSet = EmotionSet()\n",
    "        emotionSet.id = \"Emotions\"\n",
    "        \n",
    "        if self._ESTIMATION == 'Probabilities':\n",
    "            emotionSet.onyx__maxIntensityValue = float(100.0)\n",
    "        \n",
    "        emotion1 = Emotion() \n",
    "        for dimension in ['V','A','D']:\n",
    "            weights = [feature_text[i] for i in feature_text if (i != 'surprise')]\n",
    "            if not all(v == 0 for v in weights):\n",
    "                value = np.average([self.centroids[i][dimension] for i in feature_text if (i != 'surprise')], weights=weights) \n",
    "            else:\n",
    "                value = 5.0\n",
    "            emotion1[self.centroid_mappings[dimension]] = value         \n",
    "\n",
    "        emotionSet.onyx__hasEmotion.append(emotion1)    \n",
    "        \n",
    "        for i in feature_text:\n",
    "            if self._ESTIMATION == 'Probabilities':\n",
    "                emotionSet.onyx__hasEmotion.append(Emotion(\n",
    "                        onyx__hasEmotionCategory=self.wnaffect_mappings[i],\n",
    "                        onyx__hasEmotionIntensity=float(feature_text[i])*100 ))\n",
    "            elif self._ESTIMATION == 'Classes':\n",
    "                if feature_text[i] > 0:\n",
    "                    emotionSet.onyx__hasEmotion.append(Emotion(\n",
    "                        onyx__hasEmotionCategory = self.wnaffect_mappings[i]))                    \n",
    "#                         onyx__hasEmotionIntensity=int(feature_text[i])))\n",
    "        \n",
    "        entry.emotions = [emotionSet,]        \n",
    "        response.entries.append(entry)\n",
    "            \n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
