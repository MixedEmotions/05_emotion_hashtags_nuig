{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import logging\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from senpy.plugins import EmotionPlugin, SenpyPlugin\n",
    "from senpy.models import Results, EmotionSet, Entry, Emotion\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# my packages\n",
    "import codecs, csv, re, nltk\n",
    "import numpy as np\n",
    "import math, itertools\n",
    "from drevicko.twitter_regexes import cleanString, setupRegexes, tweetPreprocessor\n",
    "import preprocess_twitter\n",
    "from collections import defaultdict\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn.svm import SVC, SVR\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.tokenize.casual as casual\n",
    "\n",
    "import gzip\n",
    "from datetime import datetime \n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "class hashTagProba(EmotionPlugin):\n",
    "    \n",
    "    def __init__(self, info, *args, **kwargs):\n",
    "        super(hashTagProba, self).__init__(info, *args, **kwargs)\n",
    "        self.name = info['name']\n",
    "        self.id = info['module']\n",
    "        self._info = info\n",
    "        local_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        self._categories = {'sadness':[],\n",
    "                            'disgust':[],\n",
    "                            'surprise':[],\n",
    "                            'anger':[],\n",
    "                            'fear':[],\n",
    "                            'joy':[]}   \n",
    "\n",
    "        self._wnaffect_mappings = {'sadness':'sadness',\n",
    "                                    'disgust':'disgust',\n",
    "                                    'surprise':'surprise',\n",
    "                                    'anger':'anger',\n",
    "                                    'fear':'fear',\n",
    "                                    'joy':'joy'}\n",
    "        self._maxlen = 65\n",
    "        \n",
    "        self._paths = {\n",
    "            \"word_emb\": \"glove.twitter.27B.100d.txt\",\n",
    "            \"word_freq\": 'wordFrequencies.dump',\n",
    "            \"classifiers\" : 'classifiers',            \n",
    "            \"ngramizers\": 'ngramizers'\n",
    "            }\n",
    "        \n",
    "        self._savedModelPath = local_path + \"/classifiers/LSTM/hashTagProba\"\n",
    "        self._path_wordembeddings = os.path.dirname(local_path) + '/glove.twitter.27B.100d.txt.gz'\n",
    "        \n",
    "        self._emoNames = ['sadness', 'disgust', 'surprise', 'anger', 'fear', 'joy'] \n",
    "#         self._emoNames = ['anger','fear','joy','sadness'] \n",
    "        \n",
    "        self._classifiers = {}\n",
    "\n",
    "        self._Dictionary = {}\n",
    "        \n",
    "        self.centroids= {\n",
    "                            \"anger\": {\n",
    "                                \"A\": 6.95, \n",
    "                                \"D\": 5.1, \n",
    "                                \"V\": 2.7}, \n",
    "                            \"disgust\": {\n",
    "                                \"A\": 5.3, \n",
    "                                \"D\": 8.05, \n",
    "                                \"V\": 2.7}, \n",
    "                            \"fear\": {\n",
    "                                \"A\": 6.5, \n",
    "                                \"D\": 3.6, \n",
    "                                \"V\": 3.2}, \n",
    "                            \"joy\": {\n",
    "                                \"A\": 7.22, \n",
    "                                \"D\": 6.28, \n",
    "                                \"V\": 8.6}, \n",
    "                            \"sadness\": {\n",
    "                                \"A\": 5.21, \n",
    "                                \"D\": 2.82, \n",
    "                                \"V\": 2.21},\n",
    "                            \"neutral\": {\n",
    "                                \"A\": 5.0, \n",
    "                                \"D\": 5.0, \n",
    "                                \"V\": 5.0\n",
    "                            }\n",
    "                        }        \n",
    "        self.emotions_ontology = {\n",
    "            \"anger\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#anger\", \n",
    "            \"disgust\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#disgust\", \n",
    "            \"fear\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#negative-fear\", \n",
    "            \"joy\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#joy\", \n",
    "            \"neutral\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#neutral-emotion\",             \n",
    "            \"sadness\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#sadness\"\n",
    "            }\n",
    "        \n",
    "        self._centroid_mappings = {\n",
    "            \"V\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#valence\",\n",
    "            \"A\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#arousal\",\n",
    "            \"D\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#dominance\"          \n",
    "            }\n",
    "        self._blank = [0] * len(self._emoNames)\n",
    "        \n",
    "\n",
    "    def activate(self, *args, **kwargs):\n",
    "        \n",
    "        np.random.seed(1337)\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._hashTagDLModel = self._load_model_and_weights(self._savedModelPath)  \n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _hashTagDLModel\"))\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._Dictionary, self._Indices = self._load_original_vectors(\n",
    "            filename = self._path_wordembeddings, \n",
    "            sep = ' ',\n",
    "            wordFrequencies = None, \n",
    "            zipped = True) # leave wordFrequencies=None for loading the entire WE file\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordEmbeddings\"))\n",
    "        \n",
    "        logger.info(\"hashTagProba plugin is ready to go!\")\n",
    "        \n",
    "    def deactivate(self, *args, **kwargs):\n",
    "        try:\n",
    "            logger.info(\"hashTagProba plugin is being deactivated...\")\n",
    "        except Exception:\n",
    "            print(\"Exception in logger while reporting deactivation of hashTagProba\")\n",
    "\n",
    "    #MY FUNCTIONS\n",
    "    \n",
    "    def _load_model_and_weights(self, filename):\n",
    "        with open(filename+'.json', 'r') as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            \n",
    "        loaded_model.load_weights(filename+'.h5')\n",
    "        \n",
    "        return loaded_model\n",
    "    \n",
    "    def _lists_to_vectors(self, text):\n",
    "\n",
    "        train_sequences = [self._text_to_sequence(text)]  \n",
    "        X = sequence.pad_sequences(train_sequences, maxlen=self._maxlen)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _text_to_sequence(self,text):\n",
    "\n",
    "        train_sequence = []\n",
    "        for token in text.split():\n",
    "            try:\n",
    "                train_sequence.append(self._Indices[token])\n",
    "            except:\n",
    "                train_sequence.append(0)\n",
    "        train_sequence.extend([0]*( self._maxlen-len(train_sequence)) )\n",
    "        return np.array(train_sequence)  \n",
    "    \n",
    "    def _text_preprocessor(self, text):\n",
    "        \n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        \n",
    "        text = casual.reduce_lengthening(text)\n",
    "        text = cleanString(setupRegexes('twitterProAna'),text)  \n",
    "        text = ' '.join([span for notentity,span in tweetPreprocessor(text, (\"urls\", \"users\", \"lists\")) if notentity]) \n",
    "        text = text.replace('\\t','')\n",
    "        text = text.replace('< ','<').replace(' >','>')\n",
    "        text = text.replace('):', '<sadface>').replace('(:', '<smile>')\n",
    "        text = text.replace(\" 't\", \"t\")#.replace(\"#\", \"\")\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def tokenise_tweet(text):\n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        text = preprocess_tweet(text)     \n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    \n",
    "    def _load_original_vectors(self, filename = 'glove.27B.100d.txt', sep = ' ', wordFrequencies = None, zipped = False):\n",
    "       \n",
    "        def __read_file(f):\n",
    "            Dictionary, Indices  = {},{}\n",
    "            i = 1\n",
    "            for line in f:\n",
    "                line_d = line.decode('utf-8').split(sep)\n",
    "\n",
    "                token = line_d[0]\n",
    "                token_vector = np.array(line_d[1:], dtype = 'float32')   \n",
    "                if(wordFrequencies):\n",
    "                    if(token in wordFrequencies):                \n",
    "                        Dictionary[token] = token_vector\n",
    "                        Indices.update({token:i})\n",
    "                        i+=1\n",
    "                else:\n",
    "                    Dictionary[token] = token_vector\n",
    "                    Indices.update({token:i})\n",
    "                    i+=1\n",
    "            return(Dictionary, Indices)\n",
    "            \n",
    "        if zipped:\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                return(__read_file(f))\n",
    "        else:\n",
    "            with open(filename, 'rb') as f:\n",
    "                return(__read_file(f))\n",
    "            \n",
    "\n",
    "    def _extract_features(self, X):\n",
    "        if self._ESTIMATION == 'Probabilities':            \n",
    "            y_predict = np.array(self._hashTagDLModel.predict(X))[0]\n",
    "            \n",
    "        else:\n",
    "            blank = self._blank\n",
    "            for i,pred in enumerate(self._hashTagDLModel.predict_classes(X)):\n",
    "                print(i,pred)\n",
    "                blank[pred] = 1\n",
    "            y_predict = np.array(blank)\n",
    "        feature_set = {emo: y_ for emo, y_ in zip(self._emoNames, y_predict)}\n",
    "            \n",
    "        return feature_set       \n",
    "    \n",
    "    \n",
    "    def analyse(self, **params):\n",
    "        logger.debug(\"Hashtag LSTM Analysing with params {}\".format(params))          \n",
    "        \n",
    "        st = datetime.now()\n",
    "           \n",
    "        text_input = params.get(\"input\", None)\n",
    "        \n",
    "        self._ESTIMATION = params.get(\"estimation\", 'Probabilities')\n",
    "        text = self._text_preprocessor(text_input)    \n",
    "        \n",
    "        X = self._lists_to_vectors(text = text)   \n",
    "        \n",
    "        feature_text = self._extract_features(X = X)    \n",
    "        \n",
    "            \n",
    "        response = Results()\n",
    "       \n",
    "        entry = Entry()\n",
    "        entry.nif__isString = text_input\n",
    "        \n",
    "        emotionSet = EmotionSet()\n",
    "        emotionSet.id = \"Emotions\"\n",
    "        \n",
    "        if self._ESTIMATION == 'Probabilities':\n",
    "            emotionSet.onyx__maxIntensityValue = float(100.0)\n",
    "        \n",
    "        emotion1 = Emotion() \n",
    "        for dimension in ['V','A','D']:\n",
    "            weights = [feature_text[i] for i in feature_text if (i != 'surprise')]\n",
    "            if not all(v == 0 for v in weights):\n",
    "                value = np.average([self.centroids[i][dimension] for i in feature_text if (i != 'surprise')], weights=weights) \n",
    "            else:\n",
    "                value = 5.0\n",
    "            emotion1[self._centroid_mappings[dimension]] = value         \n",
    "\n",
    "        emotionSet.onyx__hasEmotion.append(emotion1)    \n",
    "        \n",
    "        for i in feature_text:\n",
    "            if self._ESTIMATION == 'Probabilities':\n",
    "                emotionSet.onyx__hasEmotion.append(Emotion(\n",
    "                        onyx__hasEmotionCategory=self._wnaffect_mappings[i],\n",
    "                        onyx__hasEmotionIntensity=float(feature_text[i])*100 ))\n",
    "            elif self._ESTIMATION == 'Classes':\n",
    "                if(feature_text[i] > 0):\n",
    "                    emotionSet.onyx__hasEmotion.append(Emotion(\n",
    "                        onyx__hasEmotionCategory=self._wnaffect_mappings[i]))\n",
    "#                         onyx__hasEmotionIntensity=int(feature_text[i])))\n",
    "        \n",
    "        entry.emotions = [emotionSet,]\n",
    "        \n",
    "        response.entries.append(entry)\n",
    "        \n",
    "        \n",
    "        # entry.language = lang\n",
    "            \n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
