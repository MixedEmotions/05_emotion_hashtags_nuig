{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported regex as re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import logging\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from senpy.plugins import EmotionPlugin, SenpyPlugin\n",
    "from senpy.models import Results, EmotionSet, Entry, Emotion\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# my packages\n",
    "import codecs, csv, re, nltk\n",
    "import numpy as np\n",
    "import math, itertools\n",
    "from drevicko.twitter_regexes import cleanString, setupRegexes, tweetPreprocessor\n",
    "import preprocess_twitter\n",
    "from collections import defaultdict\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn.svm import SVC, SVR\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk.tokenize.casual as casual\n",
    "\n",
    "import gzip\n",
    "from datetime import datetime \n",
    "\n",
    "import random\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "class fivePointRegression(EmotionPlugin):\n",
    "    \n",
    "    def __init__(self, info, *args, **kwargs):\n",
    "        super(fivePointRegression, self).__init__(info, *args, **kwargs)\n",
    "        self.name = info['name']\n",
    "        self.id = info['module']\n",
    "        self._info = info\n",
    "        local_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        self._categories = {'sadness':[],\n",
    "                            'disgust':[],\n",
    "                            'surprise':[],\n",
    "                            'anger':[],\n",
    "                            'fear':[],\n",
    "                            'joy':[]}   \n",
    "\n",
    "        self._wnaffect_mappings = {'sadness':'sadness',\n",
    "                                    'disgust':'disgust',\n",
    "                                    'surprise':'surprise',\n",
    "                                    'anger':'anger',\n",
    "                                    'fear':'fear',\n",
    "                                    'joy':'joy'}\n",
    "        \n",
    "        self._vad_mappings = {'confident':'D',\n",
    "                              'excited':'A',\n",
    "                              'happy':'V', \n",
    "                              'surprised':'S'}\n",
    "        \n",
    "        self._maxlen = 65\n",
    "        \n",
    "        self._paths = {\n",
    "            \"word_emb\": \"glove.twitter.27B.100d.txt\",\n",
    "            \"word_freq\": 'wordFrequencies.dump',\n",
    "            \"classifiers\" : 'classifiers',            \n",
    "            \"ngramizers\": 'ngramizers'\n",
    "            }\n",
    "        \n",
    "        self._savedModelPath = local_path + \"/classifiers/LSTM/fivePointRegression\"\n",
    "        self._path_wordembeddings = os.path.dirname(local_path) + '/glove.twitter.27B.100d.txt.gz'\n",
    "        \n",
    "        self._emoNames = ['confident','excited','happy', 'surprised']\n",
    "#         self._emoNames = ['sadness', 'disgust', 'surprise', 'anger', 'fear', 'joy'] \n",
    "#         self._emoNames = ['anger','fear','joy','sadness'] \n",
    "        \n",
    "        \n",
    "        self.centroids= {\n",
    "                            \"anger\": {\n",
    "                                \"A\": 6.95, \n",
    "                                \"D\": 5.1, \n",
    "                                \"V\": 2.7}, \n",
    "                            \"disgust\": {\n",
    "                                \"A\": 5.3, \n",
    "                                \"D\": 8.05, \n",
    "                                \"V\": 2.7}, \n",
    "                            \"fear\": {\n",
    "                                \"A\": 6.5, \n",
    "                                \"D\": 3.6, \n",
    "                                \"V\": 3.2}, \n",
    "                            \"joy\": {\n",
    "                                \"A\": 7.22, \n",
    "                                \"D\": 6.28, \n",
    "                                \"V\": 8.6}, \n",
    "                            \"sadness\": {\n",
    "                                \"A\": 5.21, \n",
    "                                \"D\": 2.82, \n",
    "                                \"V\": 2.21}\n",
    "                        }        \n",
    "        self.emotions_ontology = {\n",
    "            \"anger\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#anger\", \n",
    "            \"disgust\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#disgust\", \n",
    "            \"fear\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#negative-fear\", \n",
    "            \"joy\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#joy\", \n",
    "            \"neutral\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#neutral-emotion\",             \n",
    "            \"sadness\": \"http://gsi.dit.upm.es/ontologies/wnaffect/ns#sadness\"\n",
    "            }\n",
    "        \n",
    "        self._centroid_mappings = {\n",
    "            \"V\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#valence\",\n",
    "            \"A\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#arousal\",\n",
    "            \"D\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#dominance\",\n",
    "            \"S\": \"http://www.gsi.dit.upm.es/ontologies/onyx/vocabularies/anew/ns#surprise\"\n",
    "            }\n",
    "        \n",
    "\n",
    "    def activate(self, *args, **kwargs):\n",
    "        \n",
    "        np.random.seed(1337)\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._fivePointRegressionModel = self._load_model_and_weights(self._savedModelPath)  \n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _fivePointRegressionModel\"))\n",
    "        \n",
    "        st = datetime.now()\n",
    "        self._Dictionary, self._Indices = self._load_original_vectors(\n",
    "            filename = self._path_wordembeddings, \n",
    "            sep = ' ',\n",
    "            wordFrequencies = None, \n",
    "            zipped = True) # leave wordFrequencies=None for loading the entire WE file\n",
    "        logger.info(\"{} {}\".format(datetime.now() - st, \"loaded _wordEmbeddings\"))\n",
    "        \n",
    "        logger.info(\"fivePointRegression plugin is ready to go!\")\n",
    "        \n",
    "    def deactivate(self, *args, **kwargs):\n",
    "        try:\n",
    "            logger.info(\"fivePointRegression plugin is being deactivated...\")\n",
    "        except Exception:\n",
    "            print(\"Exception in logger while reporting deactivation of fivePointRegression\")\n",
    "\n",
    "    #MY FUNCTIONS\n",
    "    \n",
    "    def _load_model_and_weights(self, filename):\n",
    "        with open(filename+'.json', 'r') as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            \n",
    "        loaded_model.load_weights(filename+'.h5')\n",
    "        \n",
    "        return loaded_model\n",
    "    \n",
    "    def _lists_to_vectors(self, text):\n",
    "\n",
    "        train_sequences = [self._text_to_sequence(text)]  \n",
    "        X = sequence.pad_sequences(train_sequences, maxlen=self._maxlen)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _text_to_sequence(self,text):\n",
    "\n",
    "        train_sequence = []\n",
    "        for token in text.split():\n",
    "            try:\n",
    "                train_sequence.append(self._Indices[token])\n",
    "            except:\n",
    "                train_sequence.append(0)\n",
    "        train_sequence.extend([0]*( self._maxlen-len(train_sequence)) )\n",
    "        return np.array(train_sequence)  \n",
    "    \n",
    "    def _text_preprocessor(self, text):\n",
    "        \n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        \n",
    "        text = casual.reduce_lengthening(text)\n",
    "        text = cleanString(setupRegexes('twitterProAna'),text)  \n",
    "        text = ' '.join([span for notentity,span in tweetPreprocessor(text, (\"urls\", \"users\", \"lists\")) if notentity]) \n",
    "        text = text.replace('\\t','')\n",
    "        text = text.replace('< ','<').replace(' >','>')\n",
    "        text = text.replace('):', '<sadface>').replace('(:', '<smile>')\n",
    "        text = text.replace(\" 't\", \"t\")#.replace(\"#\", \"\")\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def tokenise_tweet(text):\n",
    "        text = preprocess_twitter.tokenize(text)\n",
    "        text = preprocess_tweet(text)     \n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    \n",
    "    def _load_original_vectors(self, filename = 'glove.27B.100d.txt', sep = ' ', wordFrequencies = None, zipped = False):\n",
    "       \n",
    "        def __read_file(f):\n",
    "            Dictionary, Indices  = {},{}\n",
    "            i = 1\n",
    "            for line in f:\n",
    "                line_d = line.decode('utf-8').split(sep)\n",
    "\n",
    "                token = line_d[0]\n",
    "                token_vector = np.array(line_d[1:], dtype = 'float32')   \n",
    "                if(wordFrequencies):\n",
    "                    if(token in wordFrequencies):                \n",
    "                        Dictionary[token] = token_vector\n",
    "                        Indices.update({token:i})\n",
    "                        i+=1\n",
    "                else:\n",
    "                    Dictionary[token] = token_vector\n",
    "                    Indices.update({token:i})\n",
    "                    i+=1\n",
    "            return(Dictionary, Indices)\n",
    "            \n",
    "        if zipped:\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                return(__read_file(f))\n",
    "        else:\n",
    "            with open(filename, 'rb') as f:\n",
    "                return(__read_file(f))\n",
    "            \n",
    "\n",
    "    def _extract_features(self, X):\n",
    "#         if self._ESTIMATION == 'Probabilities':            \n",
    "#             y_predict = np.array(self._fivePointRegressionModel.predict(X))[0]            \n",
    "#         else:\n",
    "#             y_predict = np.array([self._blank[y_] for y_ in self._fivePointRegressionModel.predict_classes(X)][0])\n",
    "        y_predict = np.array(self._fivePointRegressionModel.predict(X))[0]\n",
    "        feature_set = {self._vad_mappings[emo]:float(y_) for emo, y_ in zip(self._emoNames, y_predict)}\n",
    "            \n",
    "        return feature_set       \n",
    "    \n",
    "    # CONVERSION EKMAN TO VAD\n",
    "\n",
    "    \n",
    "    def _backwards_conversion(self, original):    \n",
    "        \"\"\"Find the closest category\"\"\"\n",
    "\n",
    "        dimensions = list(self.centroids.values())[0]\n",
    "\n",
    "        def distance(e1, e2):\n",
    "            return sum((e1[k] - e2.get(k, 0)) for k in dimensions)\n",
    "\n",
    "        distances = { state:distance(self.centroids[state], original) for state in self.centroids }\n",
    "        mindistance = max(distances.values())\n",
    "\n",
    "        for state in distances:\n",
    "            if distances[state] < mindistance:\n",
    "                mindistance = distances[state]\n",
    "                emotion = state\n",
    "\n",
    "        result = Emotion(onyx__hasEmotionCategory=emotion)\n",
    "        return result\n",
    "       \n",
    "    \n",
    "    def analyse(self, **params):\n",
    "        logger.debug(\"fivePointRegression LSTM Analysing with params {}\".format(params))          \n",
    "        \n",
    "        st = datetime.now()           \n",
    "        text_input = params.get(\"input\", None)\n",
    "        \n",
    "        text = self._text_preprocessor(text_input)            \n",
    "        X = self._lists_to_vectors(text = text)           \n",
    "        feature_text = self._extract_features(X = X)    \n",
    "        \n",
    "            \n",
    "        response = Results()       \n",
    "        entry = Entry()\n",
    "        entry.nif__isString = text_input\n",
    "        \n",
    "        emotionSet = EmotionSet()\n",
    "        emotionSet.id = \"Emotions\"\n",
    "        \n",
    "        emotion = Emotion() \n",
    "        for dimension in [\"V\",\"A\",\"D\",\"S\"]:\n",
    "#             emotion[self._centroid_mappings[dimension]] = float((2+feature_text[dimension])*2.5) \n",
    "            emotion[dimension] = float(feature_text[dimension]*10) \n",
    "    \n",
    "        emotionSet.onyx__hasEmotion.append(emotion)  \n",
    "#         emotionSet.onyx__hasEmotion.append(self._backwards_conversion(emotion))\n",
    "        \n",
    "        \"\"\"\n",
    "        for semeval\n",
    "        \n",
    "        \n",
    "        \n",
    "        dimensions = list(self.centroids.values())[0]\n",
    "\n",
    "        def distance(e1, e2):\n",
    "            return sum((e1[k] - e2.get(k, 0)) for k in dimensions)\n",
    "\n",
    "        distances = { state:distance(self.centroids[state], emotion) for state in self.centroids }\n",
    "        mindistance = max(distances.values())\n",
    "        \n",
    "        dummyfix = sorted(distances.values(),reverse=True)\n",
    "\n",
    "        for state in distances:\n",
    "            if state != 'joy':\n",
    "                if distances[state] in dummyfix[0:3]:\n",
    "                    emotionSet.onyx__hasEmotion.append(\n",
    "                        Emotion(\n",
    "                            onyx__hasEmotionCategory = state, \n",
    "                            onyx__hasEmotionIntensity = int(1))) \n",
    "                else:\n",
    "                    emotionSet.onyx__hasEmotion.append(\n",
    "                        Emotion(\n",
    "                            onyx__hasEmotionCategory = state, \n",
    "                            onyx__hasEmotionIntensity = int(0))) \n",
    "                \n",
    "        emotionSet.onyx__hasEmotion.append(\n",
    "                    Emotion(\n",
    "                        onyx__hasEmotionCategory = 'surprise', \n",
    "                        onyx__hasEmotionIntensity = float((2+feature_text['S'])/4)))\n",
    "        emotionSet.onyx__hasEmotion.append(\n",
    "                    Emotion(\n",
    "                        onyx__hasEmotionCategory = 'joy', \n",
    "                        onyx__hasEmotionIntensity = float((2+feature_text['V'])/4)))\n",
    "        \n",
    "        emotionSet.prov__wasGeneratedBy = self.id\n",
    "        \n",
    "        \n",
    "        for semeval\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        entry.emotions = [emotionSet,]        \n",
    "        response.entries.append(entry)\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# centroids= {\n",
    "#                             \"anger\": {\n",
    "#                                 \"A\": 6.95, \n",
    "#                                 \"D\": 5.1, \n",
    "#                                 \"V\": 2.7}, \n",
    "#                             \"disgust\": {\n",
    "#                                 \"A\": 5.3, \n",
    "#                                 \"D\": 8.05, \n",
    "#                                 \"V\": 2.7}, \n",
    "#                             \"fear\": {\n",
    "#                                 \"A\": 6.5, \n",
    "#                                 \"D\": 3.6, \n",
    "#                                 \"V\": 3.2}, \n",
    "#                             \"joy\": {\n",
    "#                                 \"A\": 7.22, \n",
    "#                                 \"D\": 6.28, \n",
    "#                                 \"V\": 8.6}, \n",
    "#                             \"sadness\": {\n",
    "#                                 \"A\": 5.21, \n",
    "#                                 \"D\": 2.82, \n",
    "#                                 \"V\": 2.21}\n",
    "#                         }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def _backwards_conversion(original):    \n",
    "#         \"\"\"Find the closest category\"\"\"\n",
    "        \n",
    "#         dimensions = list(centroids.values())[0]\n",
    "        \n",
    "#         def distance(e1, e2):\n",
    "#             return sum((e1[k] - e2.get(k, 0)) for k in dimensions)\n",
    "        \n",
    "#         def _vectors_similarity(v1 , v2):\n",
    "#             return( 1 - spatial.distance.cosine(v1,v2) )\n",
    "\n",
    "#         distances = { state:abs(distance(centroids[state], original)) for state in centroids }\n",
    "#         print(np.array(centroids['anger'].values()))\n",
    "#         distances2 = {state:_vectors_similarity(centroids[state].values() , feature_text.values())  for state in centroids}\n",
    "#         mindistance = max(distances.values())\n",
    "#         print(distances)\n",
    "#         print(distances2)\n",
    "#         for state in distances:\n",
    "#             if distances[state] < mindistance:\n",
    "#                 mindistance = distances[state]\n",
    "#                 emotion = state\n",
    "                \n",
    "#         result = Emotion(onyx__hasEmotionCategory=emotion, onyx__hasEmotionIntensity=emotion)\n",
    "#         return result\n",
    "    \n",
    "# feature_text = {\n",
    "#     \"A\":5.9574053436517715,\n",
    "#     \"D\":6.3352929055690765,\n",
    "#     \"V\":2.9072564840316772\n",
    "\n",
    "# }\n",
    "\n",
    "# import numpy as np\n",
    "# from senpy.models import Emotion\n",
    "# from scipy import spatial\n",
    "\n",
    "# emotion = Emotion() \n",
    "# for dimension in [\"V\",\"A\",\"D\"]:\n",
    "#     emotion[dimension] = float((feature_text[dimension])) \n",
    "    \n",
    "# _backwards_conversion(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n",
      "0.913783953235\n",
      "anger\n",
      "0.984693951883\n",
      "sadness\n",
      "0.954294107704\n",
      "fear\n",
      "0.954861071587\n",
      "disgust\n",
      "0.985234467968\n"
     ]
    }
   ],
   "source": [
    "# for state in centroids:\n",
    "# #     print(centroids[state])\n",
    "# #     print([i for i in feature_text.values()])\n",
    "# #     print(([i for i in centroids[state].values()]))\n",
    "#     print(state)\n",
    "#     print(_vectors_similarity(\n",
    "#             [i for i in feature_text.values()],\n",
    "#             [i for i in centroids[state].values()]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
