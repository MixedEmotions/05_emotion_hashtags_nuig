{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Regular Expressions\n",
    "Regular expressions for pre-processing pro-ana tweets. Includes:\n",
    "\n",
    " - surrounding LIWC-aware punctuation with spaces\n",
    " - (except for url's and twitter entities)\n",
    " - regexes for assorted smileys into 4 tokens: ): (: (; :/\n",
    " - case munging: lower case unless all caps\n",
    " \n",
    "The format is a list of `('search expression','replacement expression')` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported regex as re\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "try:\n",
    "    re\n",
    "except NameError:\n",
    "    import regex as re\n",
    "    print (\"imported regex as re\")\n",
    "\n",
    "try:\n",
    "    from ttp import ttp\n",
    "except ImportError:\n",
    "    ttp = None\n",
    "    print(\"Couldn't load twitter-text! Tweet entities may not be recognised! Try `pip install twitter-text-python`\")\n",
    "\n",
    "try:\n",
    "    # Python 2.6-2.7 \n",
    "    from HTMLParser import HTMLParser\n",
    "    unescape = HTMLParser().unescape\n",
    "except ImportError:\n",
    "    # Python 3.5\n",
    "    from html import unescape\n",
    "\n",
    "from itertools import chain # is this the same in python 2?\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuationRe = r\"\"\" (['`\\[\\](){}⟨⟩:,\\-‒–—―!.?‘’“”\"•;…/\\|=+_~@#^&*<>]) \"\"\"\n",
    "\n",
    "regexStyles = [\"twitterProAna\",\"wordsAndPunct\",\"walworth\"]\n",
    "def setupRegexes(style=\"twitterProAna\"):\n",
    "  if style == regexStyles[0]: # twitterProAna or twitterEmoji\n",
    "    def separateNumbers(m):\n",
    "        if separateNumbers.string != m.string:\n",
    "            separateNumbers.string = m.string\n",
    "            separateNumbers.notParsing = False\n",
    "        s=m.group()\n",
    "        if s in set(('http','@','#')):\n",
    "            separateNumbers.notParsing = True\n",
    "            return s\n",
    "        if s == ' ':\n",
    "            separateNumbers.notParsing = False\n",
    "            return s\n",
    "        if separateNumbers.notParsing:\n",
    "            return s\n",
    "        return ' '+s+' '\n",
    "    separateNumbers.string = None\n",
    "    separateNumbers.searchString = r\"http|@|#|\\d+|\\s\"\n",
    "    \n",
    "    def skipEntities(m):\n",
    "        \n",
    "        return ' '+s+' '\n",
    "    LIWC_punct = r\"\"\"'`\\[\\](){}⟨⟩:,\\-‒–—―!.?‘’“”\"•;…/\\|=+_~@#^&*<>\"\"\"\n",
    "    cleaningReList = [ \\\n",
    "                      # surround punctuation with spaces (needs regex module, re module doesn't work!)\n",
    "    (r\"(?V1)([\"+LIWC_punct+\"||[^\\p{Letter}\\p{Separator}\\p{Number}]])\",r\" \\1 \"),\n",
    "                      # standardise quote characters\n",
    "    (u\"['`‘’“”\\\"]\",\"'\"),\n",
    "                      # standardise smileys\n",
    "    (r\" [:=;] ( [\\-Do'`‘’xPpLCc\"'\"'r\"/,~] )?( [(\\[{] )+\",\" ): \"),\n",
    "    (r\" [:=] ( [\\-Do'`‘’xPpLCc\"'\"'r\"/,~] )?( [)\\]}] )+\",\" (: \"),\n",
    "    (r\" ; ( [\\-Do'`‘’xPpLCc\"'\"'r\"/~] )?( [)\\]}] )+|( [(\\[{] )+( [\\-Do'`‘’xPpLCc\"'\"'r\"/~] )? ; \",\" (; \"),\n",
    "    (r\"(?<!http)(?<!https) [:=] ( [\\-Do'`‘’xPpLCc\"'\"'r\"/,~] )? / ( / )*\",\" :/ \"),\n",
    "    (r\" - ( [._] )+ - \",\" ): \"),\n",
    "    (r\"( [(\\[{] )+( [\\-Do'`‘’xPpLCc\"'\"'r\"/,~] )? [:=;] \",\" (: \"),\n",
    "    (r\"( [)\\]}] )+( [\\-Do'`‘’xPpLCc\"'\"'r\"/,~] )? [:=] \",\" ): \"),\n",
    "                      # reform heart emoticons\n",
    "    (r\"< 3\",\"<3\"),\n",
    "    #(r\"http :  /  / (\\S*)\",r\"http://\\1\"),\n",
    "                      # reform url's (ie: remove inserted spaces)\n",
    "    (r\"http(s)? :  /  / ((\\S*) (\\.) |(\\S*) (/) )(\\S*)\",r\"http\\1://\\3\\4\\5\\6\\7\"),\n",
    "    (r\"http(s)?://(\\S*)( (\\.) (\\S*)| (/) (\\S*))\",r\"http\\1://\\2\\4\\5\\6\\7\"),\n",
    "    (r\"http(s)?://(\\S*)( (\\.) (\\S*)| (/) (\\S*))\",r\"http\\1://\\2\\4\\5\\6\\7\"),\n",
    "    #(r\"(?<=[_\\W0-9])['`‘’]|['`‘’](?=[_\\W0-9])\",\" ' \"),\n",
    "    #(r\"(?<=[a-zA-Z])(?=\\d)|(?<=\\d)(?=[a-zA-Z])\",r\" \"),\n",
    "                      # keep \"can't\" etc as two tokens \"can\" and \"'t\"\n",
    "                      # TODO: !! don't should become 'do' and 't\n",
    "    (r\"(\\w) ' (\\w)\",r\"\\1 '\\2\"),\n",
    "                      # keep \n",
    "                      # separate words and numbers (when not in a url)\n",
    "    (r\"([#@]) (\\w+)\",r\"\\1\\2\"),\n",
    "    (r\"([#@]\\w+) _ (\\w+)\",r\"\\1_\\2\"), # up to 5 underscores in a name or tag - I guess others are rare!\n",
    "    (r\"([#@]\\w+) _ (\\w+)\",r\"\\1_\\2\"),\n",
    "    (r\"([#@]\\w+) _ (\\w+)\",r\"\\1_\\2\"),\n",
    "    (r\"([#@]\\w+) _ (\\w+)\",r\"\\1_\\2\"),\n",
    "    (r\"([#@]\\w+) _ (\\w+)\",r\"\\1_\\2\"),\n",
    "    (separateNumbers.searchString,separateNumbers),\n",
    "                      # reform \"<3\" (heart symbol in text)\n",
    "    (r\" < *3 \",\" <3 \"),\n",
    "#     (r\"(\\S*)((?<=[a-zA-Z])(?=\\d)|(?<=\\d)(?=[a-zA-Z]))\",lambda x:x.group()+('' if re.findall('http',x.group()) else ' ')),\n",
    "                      # munge word case (ie. convert to lower case if not all caps) except if a url\n",
    "    (r\"\\S*([a-z]'?[A-Z]|[A-Z]'?[a-z])\\w*\",lambda x:x.group() if re.findall('http',x.group()) else x.group().lower())\n",
    "#     (r\"([^\\p{Letter}\\p{Separator}\"+LIWC_punct+\"])\",r\" \\1 \")\\\n",
    "#     (r\"([^\\w\\s\"+LIWC_punct+\"])\",r\" \\1 \")\\\n",
    "    ]\n",
    "  elif style == regexStyles[1] or style == regexStyles[2] : # \"wordsAndPunct\" or \"walworth\"\n",
    "    cleaningReList = [\\\n",
    "      (r\"\"\"(['`\\[\\](){}⟨⟩:,\\-‒–—―!.?‘’“”\"•;…/\\|=+_~@#^&*<>])\"\"\",r\" \\1 \"),\n",
    "      (u\"['`‘’’“”\\\"]\",\"'\"),\n",
    "      (u\"[\\-‒–—―]\",\"-\"),\n",
    "      (r\"(?<=[a-zA-Z])(?=\\d)|(?<=\\d)(?=[a-zA-Z])\",\" \"),\n",
    "      (r\"(\\w) ' (\\w)\",r\"\\1'\\2\")\\\n",
    "    ]\n",
    "    if style == regexStyles[2]: # \"walworth\"\n",
    "      cleaningReList.append((r\"(-\\s+)+\",r\"- \"))\n",
    "      cleaningReList.append((r\".\\s*$\",r\"\"))\n",
    "  else:\n",
    "    raise ValueError(\"Possible regexStyles: %s\"%regexStyles) \n",
    "  return cleaningReList\n",
    "\n",
    "regexList = setupRegexes()\n",
    "\n",
    "def cleanString(regexList, text):\n",
    "    '''Old cleanString function to support legacy code.'''\n",
    "    text = unescape(text)\n",
    "    if type(text) is str:\n",
    "        try:\n",
    "            text = text.decode('utf-8')\n",
    "        except AttributeError:\n",
    "            pass # python 3 strings don't do 'decode', but should be ok, so no need to do it anyway\n",
    "    for regex in regexList:\n",
    "        text = re.sub(regex[0],regex[1],text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tweetParser = ttp.Parser(include_spans=True)\n",
    "except AttributeError:\n",
    "    tweetParser = None\n",
    "\n",
    "# ttpParserLookup\n",
    "    \n",
    "def tweetPreprocessor(text, entitiesToDetect=(\"urls\", \"users\", \"lists\", \"tags\")):\n",
    "    \"\"\"Takes a string, returns tuples containing either \n",
    "    (True, text_needing_parsing) or (False, entity_text_dont_parse)\n",
    "    This relies on the ttp module for parsing tweets. If that module not present, it will silently pass the \n",
    "    whole text with \"True\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        entities = tweetParser.parse(text)\n",
    "    except AttributeError:\n",
    "        return (True, text)\n",
    "    \n",
    "    spans = []\n",
    "    for label in entitiesToDetect:\n",
    "        spanList = getattr(entities, label)\n",
    "        if spanList:\n",
    "            if label == 'lists':\n",
    "                # lists are returned as a 3-tuple (name, user, (span)), we discard the user\n",
    "                spanList = [(span[0], span[2]) for span in spanList] \n",
    "            spans.extend(spanList)\n",
    "    idx = 0\n",
    "    for span in sorted(spans, key=itemgetter(1)):\n",
    "        entityStart, entityEnd = span[1]\n",
    "        startString = text[idx:entityStart]\n",
    "        if startString:\n",
    "            yield (True, startString)\n",
    "        ent = text[entityStart:entityEnd]\n",
    "        if ent:\n",
    "            yield (False, ent)\n",
    "        idx = entityEnd\n",
    "    endString = text[idx:]\n",
    "    if endString:\n",
    "        yield (True, endString)\n",
    "\n",
    "def tokenize(text, regexList=regexList, preprocessor=tweetPreprocessor):\n",
    "    \"\"\"Tokenize a string, returning an iterator over tokens as strings.\n",
    "    text : the string to be tokenized\n",
    "    regexList : a list of (regex,replaceString) tuples, defaults to tweet specific processing\n",
    "    preprocessor : a generator function preprocessor(text) which returns (boolean,substring) tuples, \n",
    "                 the boolean indicating if regexes should be applied. If None, apply regexes to original string.\n",
    "    \n",
    "    After applying regexes, the resulting string(s) are split on whitespace and yielded. Substrings returned by \n",
    "    the preprocessor with False are yielded as is (no regexes, no split)\n",
    "    \"\"\"\n",
    "    subStringIter = preprocessor(text) if preprocessor else (True, text)\n",
    "    for cleanIt, subString in subStringIter:\n",
    "        if cleanIt:\n",
    "            for word in cleanString(regexList, subString).split():\n",
    "                yield word\n",
    "        else:\n",
    "            yield subString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__doc__ = \"\"\"\n",
    "Defined functions setupRegexes(style=\\\"twitterProAna\\\"), cleanString(regexList, text) and tokenize(text).\n",
    "Available regexStyles are:\n",
    "\"\"\"\n",
    "for s in regexStyles:\n",
    "  __doc__ += \"    %s\\n\"%s"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
